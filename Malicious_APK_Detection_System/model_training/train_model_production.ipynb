{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e86803b9",
   "metadata": {},
   "source": [
    "# Production ML Model Training for Malicious APK Detection\n",
    "Supports: Drebin, CICAndMal2017, AndroZoo, and custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc6947",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2600a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, roc_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe93d9",
   "metadata": {},
   "source": [
    "## 2. Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb187d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('logs/model_training.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e7ae1",
   "metadata": {},
   "source": [
    "## 3. DatasetLoader Class\n",
    "Load various malware datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    \"\"\"Load various malware datasets\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_drebin(csv_path):\n",
    "        \"\"\"\n",
    "        Load Drebin dataset from CSV\n",
    "        Expected format: features + 'malware' label column\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading Drebin dataset from {csv_path}\")\n",
    "        try:\n",
    "            # Load with low_memory=False to handle mixed types\n",
    "            df = pd.read_csv(csv_path, low_memory=False)\n",
    "            \n",
    "            # Separate features and labels\n",
    "            if 'malware' in df.columns:\n",
    "                y_raw = df['malware']\n",
    "                X_df = df.drop('malware', axis=1)\n",
    "            elif 'class' in df.columns:\n",
    "                y_raw = df['class']\n",
    "                X_df = df.drop('class', axis=1)\n",
    "            else:\n",
    "                logger.error(\"No label column found. Expected 'malware' or 'class'\")\n",
    "                return None, None\n",
    "            \n",
    "            # Convert labels to integers (handle string/mixed types)\n",
    "            # Map common string labels to 0/1\n",
    "            if y_raw.dtype == 'object' or y_raw.dtype == 'str':\n",
    "                y_raw = y_raw.astype(str).str.lower().str.strip()\n",
    "                # Map various formats to binary\n",
    "                y_raw = y_raw.replace({\n",
    "                    'benign': 0, 'malware': 1, 'malicious': 1,\n",
    "                    'b': 0, 'm': 1, 's': 1, \n",
    "                    '0': 0, '1': 1,\n",
    "                    'false': 0, 'true': 1\n",
    "                })\n",
    "            \n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            y = pd.to_numeric(y_raw, errors='coerce')\n",
    "            \n",
    "            # Drop rows with invalid labels\n",
    "            valid_indices = ~y.isna()\n",
    "            if not valid_indices.all():\n",
    "                dropped = (~valid_indices).sum()\n",
    "                logger.warning(f\"Dropping {dropped} samples with invalid labels\")\n",
    "                y = y[valid_indices]\n",
    "                X_df = X_df[valid_indices]\n",
    "            \n",
    "            # Convert labels to int\n",
    "            y = y.astype(int).values\n",
    "            \n",
    "            # Convert features to numeric, handling mixed types\n",
    "            X_numeric = X_df.copy()\n",
    "            for col in X_numeric.columns:\n",
    "                if X_numeric[col].dtype == 'object':\n",
    "                    # Try to convert string columns to numeric\n",
    "                    X_numeric[col] = pd.to_numeric(X_numeric[col], errors='coerce')\n",
    "            \n",
    "            # Replace any remaining NaN values with 0\n",
    "            X_numeric = X_numeric.fillna(0)\n",
    "            X = X_numeric.values\n",
    "            \n",
    "            logger.info(f\"Loaded {len(X)} samples with {X.shape[1]} features\")\n",
    "            logger.info(f\"Malware samples: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
    "            logger.info(f\"Benign samples: {len(y) - sum(y)} ({(len(y) - sum(y))/len(y)*100:.1f}%)\")\n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load Drebin dataset: {e}\")\n",
    "            import traceback\n",
    "            logger.error(traceback.format_exc())\n",
    "            return None, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_cicandmal2017(data_dir):\n",
    "        \"\"\"\n",
    "        Load CICAndMal2017 dataset\n",
    "        Expected structure: data_dir/benign/*.csv and data_dir/malware/*.csv\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading CICAndMal2017 dataset from {data_dir}\")\n",
    "        try:\n",
    "            benign_files = list(Path(data_dir).glob('benign/*.csv'))\n",
    "            malware_files = list(Path(data_dir).glob('malware/*.csv'))\n",
    "            \n",
    "            benign_data = []\n",
    "            malware_data = []\n",
    "            \n",
    "            for file in benign_files:\n",
    "                df = pd.read_csv(file, low_memory=False)\n",
    "                benign_data.append(df)\n",
    "            \n",
    "            for file in malware_files:\n",
    "                df = pd.read_csv(file, low_memory=False)\n",
    "                malware_data.append(df)\n",
    "            \n",
    "            benign_df = pd.concat(benign_data, ignore_index=True)\n",
    "            malware_df = pd.concat(malware_data, ignore_index=True)\n",
    "            \n",
    "            # Create labels\n",
    "            benign_df['malware'] = 0\n",
    "            malware_df['malware'] = 1\n",
    "            \n",
    "            # Combine\n",
    "            full_df = pd.concat([benign_df, malware_df], ignore_index=True)\n",
    "            \n",
    "            y = full_df['malware'].values\n",
    "            X = full_df.drop('malware', axis=1).values\n",
    "            \n",
    "            logger.info(f\"Loaded {len(X)} samples with {X.shape[1]} features\")\n",
    "            logger.info(f\"Malware samples: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load CICAndMal2017 dataset: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_custom_csv(csv_path, label_column='label'):\n",
    "        \"\"\"\n",
    "        Load custom CSV dataset\n",
    "        Args:\n",
    "            csv_path: Path to CSV file\n",
    "            label_column: Name of label column (0=benign, 1=malware)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading custom dataset from {csv_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, low_memory=False)\n",
    "            \n",
    "            if label_column not in df.columns:\n",
    "                logger.error(f\"Label column '{label_column}' not found\")\n",
    "                return None, None\n",
    "            \n",
    "            y = df[label_column].values\n",
    "            X = df.drop(label_column, axis=1).values\n",
    "            \n",
    "            logger.info(f\"Loaded {len(X)} samples with {X.shape[1]} features\")\n",
    "            logger.info(f\"Malware samples: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load custom dataset: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_synthetic_data(n_samples=5000):\n",
    "        \"\"\"Generate synthetic data (fallback for testing)\"\"\"\n",
    "        logger.info(f\"Generating {n_samples} synthetic samples...\")\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            is_malicious = np.random.random() < 0.4\n",
    "            feature_vector = []\n",
    "            \n",
    "            # Permission features (40 features)\n",
    "            for j in range(40):\n",
    "                if is_malicious:\n",
    "                    prob = 0.6 if j < 20 else 0.3\n",
    "                else:\n",
    "                    prob = 0.2 if j < 20 else 0.1\n",
    "                feature_vector.append(1 if np.random.random() < prob else 0)\n",
    "            \n",
    "            # Component counts (4 features)\n",
    "            if is_malicious:\n",
    "                feature_vector.extend([\n",
    "                    np.random.uniform(0.3, 1.0),\n",
    "                    np.random.uniform(0.4, 1.0),\n",
    "                    np.random.uniform(0.5, 1.0),\n",
    "                    np.random.uniform(0.2, 0.8)\n",
    "                ])\n",
    "            else:\n",
    "                feature_vector.extend([\n",
    "                    np.random.uniform(0.1, 0.5),\n",
    "                    np.random.uniform(0.1, 0.4),\n",
    "                    np.random.uniform(0.1, 0.3),\n",
    "                    np.random.uniform(0.0, 0.3)\n",
    "                ])\n",
    "            \n",
    "            # Suspicious features (6 features)\n",
    "            for j in range(6):\n",
    "                prob = 0.7 if is_malicious else 0.1\n",
    "                feature_vector.append(1 if np.random.random() < prob else 0)\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            labels.append(1 if is_malicious else 0)\n",
    "        \n",
    "        logger.info(f\"Generated {n_samples} samples with {len(features[0])} features\")\n",
    "        logger.info(f\"Malware samples: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%)\")\n",
    "        return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbc373",
   "metadata": {},
   "source": [
    "## 4. ProductionModelTrainer Class\n",
    "Train production-grade malware detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56c70b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionModelTrainer:\n",
    "    \"\"\"Train production-grade malware detection model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='random_forest'):\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "    def preprocess_data(self, X, y):\n",
    "        \"\"\"Preprocess and validate data\"\"\"\n",
    "        logger.info(\"Preprocessing data...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Feature scaling for tree-based models (optional but can help)\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        logger.info(\"✓ Preprocessing completed\")\n",
    "        return X_scaled, y\n",
    "    \n",
    "    def train_model(self, X, y, hyperparameter_tuning=False):\n",
    "        \"\"\"Train the model with optional hyperparameter tuning\"\"\"\n",
    "        logger.info(f\"Training {self.model_type} model...\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training set: {len(X_train)} samples\")\n",
    "        logger.info(f\"Test set: {len(X_test)} samples\")\n",
    "        logger.info(f\"Malicious: {sum(y_train)} ({sum(y_train)/len(y_train)*100:.1f}%)\")\n",
    "        \n",
    "        if hyperparameter_tuning:\n",
    "            self.model = self._train_with_tuning(X_train, y_train)\n",
    "        else:\n",
    "            self.model = self._train_default(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        self._evaluate_model(X_test, y_test)\n",
    "        \n",
    "        # Cross-validation\n",
    "        self._cross_validate(X_train, y_train)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _train_default(self, X_train, y_train):\n",
    "        \"\"\"Train with default hyperparameters\"\"\"\n",
    "        if self.model_type == 'random_forest':\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=30,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        elif self.model_type == 'gradient_boosting':\n",
    "            model = GradientBoostingClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        logger.info(\"✓ Model training completed\")\n",
    "        return model\n",
    "    \n",
    "    def _train_with_tuning(self, X_train, y_train):\n",
    "        \"\"\"Train with hyperparameter tuning (takes longer)\"\"\"\n",
    "        logger.info(\"Starting hyperparameter tuning (this may take a while)...\")\n",
    "        \n",
    "        if self.model_type == 'random_forest':\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [20, 30, 40],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        else:\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [5, 10, 15],\n",
    "                'learning_rate': [0.01, 0.1, 0.2]\n",
    "            }\n",
    "            base_model = GradientBoostingClassifier(random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=2\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        logger.info(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        logger.info(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    def _evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\"*70)\n",
    "        logger.info(\"MODEL EVALUATION RESULTS\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        logger.info(f\"\\nAccuracy:  {accuracy*100:.2f}%\")\n",
    "        logger.info(f\"Precision: {precision*100:.2f}%\")\n",
    "        logger.info(f\"Recall:    {recall*100:.2f}%\")\n",
    "        logger.info(f\"F1-Score:  {f1*100:.2f}%\")\n",
    "        logger.info(f\"AUC-ROC:   {auc*100:.2f}%\")\n",
    "        \n",
    "        # Classification report\n",
    "        logger.info(f\"\\nClassification Report:\")\n",
    "        logger.info(\"\\n\" + classification_report(y_test, y_pred, \n",
    "                                                 target_names=['Benign', 'Malicious']))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        logger.info(f\"\\nConfusion Matrix:\")\n",
    "        logger.info(f\"                Predicted\")\n",
    "        logger.info(f\"              Benign  Malicious\")\n",
    "        logger.info(f\"Actual Benign   {cm[0][0]:5d}    {cm[0][1]:5d}\")\n",
    "        logger.info(f\"     Malicious  {cm[1][0]:5d}    {cm[1][1]:5d}\")\n",
    "        \n",
    "        # Feature importance (for tree-based models)\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            self._log_feature_importance()\n",
    "    \n",
    "    def _cross_validate(self, X_train, y_train):\n",
    "        \"\"\"Perform cross-validation\"\"\"\n",
    "        logger.info(\"\\nPerforming 5-fold cross-validation...\")\n",
    "        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5, \n",
    "                                    scoring='f1', n_jobs=-1)\n",
    "        logger.info(f\"CV F1-Scores: {cv_scores}\")\n",
    "        logger.info(f\"Average: {cv_scores.mean()*100:.2f}% (+/- {cv_scores.std()*2*100:.2f}%)\")\n",
    "    \n",
    "    def _log_feature_importance(self):\n",
    "        \"\"\"Log top important features\"\"\"\n",
    "        feature_importance = self.model.feature_importances_\n",
    "        top_20_idx = np.argsort(feature_importance)[-20:]\n",
    "        \n",
    "        logger.info(f\"\\nTop 20 Important Features:\")\n",
    "        for idx in reversed(top_20_idx):\n",
    "            logger.info(f\"  Feature {idx:3d}: {feature_importance[idx]:.4f}\")\n",
    "    \n",
    "    def save_model(self, model_path='models/malware_model.pkl'):\n",
    "        \"\"\"Save trained model and scaler\"\"\"\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        logger.info(f\"✓ Model saved to {model_path}\")\n",
    "        \n",
    "        # Save scaler\n",
    "        scaler_path = model_path.replace('.pkl', '_scaler.pkl')\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        logger.info(f\"✓ Scaler saved to {scaler_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'model_type': self.model_type,\n",
    "            'n_features': self.model.n_features_in_ if hasattr(self.model, 'n_features_in_') else None,\n",
    "            'training_date': pd.Timestamp.now().isoformat()\n",
    "        }\n",
    "        metadata_path = model_path.replace('.pkl', '_metadata.pkl')\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        logger.info(f\"✓ Metadata saved to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79785dce",
   "metadata": {},
   "source": [
    "## 5. Configuration\n",
    "Set dataset type, model type, and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56d95463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PRODUCTION MALWARE DETECTION MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  Dataset Type: drebin\n",
      "  Dataset Path: D:\\Projects\\Malicious_APK_Detection_System\\datasets\\drebin.csv\n",
      "  Model Type: random_forest\n",
      "  Hyperparameter Tuning: False\n",
      "\n",
      "✓ Dataset file found!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATASET_TYPE = 'drebin'  # Options: 'drebin', 'cicandmal2017', 'custom', 'synthetic'\n",
    "DATASET_PATH = 'D:\\\\Projects\\\\Malicious_APK_Detection_System\\\\datasets\\\\drebin.csv'  # Update with your dataset path\n",
    "MODEL_TYPE = 'random_forest'  # Options: 'random_forest', 'gradient_boosting'\n",
    "HYPERPARAMETER_TUNING = False  # Set True for production (takes longer)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRODUCTION MALWARE DETECTION MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Dataset Type: {DATASET_TYPE}\")\n",
    "print(f\"  Dataset Path: {DATASET_PATH}\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Hyperparameter Tuning: {HYPERPARAMETER_TUNING}\")\n",
    "\n",
    "# Check if dataset file exists\n",
    "import os\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"\\n⚠️  WARNING: Dataset file not found at: {DATASET_PATH}\")\n",
    "    print(f\"   Absolute path: {os.path.abspath(DATASET_PATH)}\")\n",
    "    print(f\"   Will attempt to load anyway...\")\n",
    "else:\n",
    "    print(f\"\\n✓ Dataset file found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c3eda",
   "metadata": {},
   "source": [
    "## 6. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54257d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-14 15:58:11,271 - INFO - Loading Drebin dataset from D:\\Projects\\Malicious_APK_Detection_System\\datasets\\drebin.csv\n",
      "C:\\Users\\techp\\AppData\\Local\\Temp\\ipykernel_2656\\1886928436.py:12: DtypeWarning: Columns (92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n",
      "2026-01-14 15:58:11,452 - INFO - Loaded 15036 samples with 215 features\n",
      "2026-01-14 15:58:11,452 - ERROR - Failed to load Drebin dataset: unsupported operand type(s) for +: 'int' and 'str'\n",
      "2026-01-14 15:58:11,468 - ERROR - Failed to load dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ Dataset loading failed!\n",
      "   Attempted path: D:\\Projects\\Malicious_APK_Detection_System\\datasets\\drebin.csv\n",
      "   Absolute path: D:\\Projects\\Malicious_APK_Detection_System\\datasets\\drebin.csv\n",
      "\n",
      "Possible solutions:\n",
      "   1. Check if the file exists at the specified location\n",
      "   2. Verify the path is correct (use '../datasets/drebin.csv' from models directory)\n",
      "   3. Try using synthetic data by setting: DATASET_TYPE = 'synthetic'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Dataset loading failed - check the path and try again",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   2. Verify the path is correct (use \u001b[39m\u001b[33m'\u001b[39m\u001b[33m../datasets/drebin.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m from models directory)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   3. Try using synthetic data by setting: DATASET_TYPE = \u001b[39m\u001b[33m'\u001b[39m\u001b[33msynthetic\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset loading failed - check the path and try again\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Dataset loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Dataset loading failed - check the path and try again"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "loader = DatasetLoader()\n",
    "\n",
    "if DATASET_TYPE == 'drebin':\n",
    "    X, y = loader.load_drebin(DATASET_PATH)\n",
    "elif DATASET_TYPE == 'cicandmal2017':\n",
    "    X, y = loader.load_cicandmal2017(DATASET_PATH)\n",
    "elif DATASET_TYPE == 'custom':\n",
    "    X, y = loader.load_custom_csv(DATASET_PATH)\n",
    "else:  # synthetic\n",
    "    X, y = loader.generate_synthetic_data(n_samples=10000)\n",
    "\n",
    "if X is None or y is None:\n",
    "    logger.error(\"Failed to load dataset.\")\n",
    "    print(\"\\n❌ Dataset loading failed!\")\n",
    "    print(f\"   Attempted path: {DATASET_PATH}\")\n",
    "    print(f\"   Absolute path: {os.path.abspath(DATASET_PATH)}\")\n",
    "    print(f\"\\nPossible solutions:\")\n",
    "    print(f\"   1. Check if the file exists at the specified location\")\n",
    "    print(f\"   2. Verify the path is correct (use '../datasets/drebin.csv' from models directory)\")\n",
    "    print(f\"   3. Try using synthetic data by setting: DATASET_TYPE = 'synthetic'\")\n",
    "    raise Exception(\"Dataset loading failed - check the path and try again\")\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"  Samples: {len(X)}\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Malware ratio: {sum(y)/len(y)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8342b7",
   "metadata": {},
   "source": [
    "## 7. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer instance\n",
    "trainer = ProductionModelTrainer(model_type=MODEL_TYPE)\n",
    "\n",
    "# Preprocess data\n",
    "X_processed, y_processed = trainer.preprocess_data(X, y)\n",
    "\n",
    "print(f\"\\n✓ Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e4b9e",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9fe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = trainer.train_model(\n",
    "    X_processed, \n",
    "    y_processed, \n",
    "    hyperparameter_tuning=HYPERPARAMETER_TUNING\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761af2b",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "trainer.save_model('models/malwares_model.pkl')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nModel is ready for production use.\")\n",
    "print(\"Deploy with: python run.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcceace",
   "metadata": {},
   "source": [
    "## 10. Test the Model (Optional)\n",
    "Quick test to verify the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick prediction test\n",
    "if X_processed is not None and len(X_processed) > 0:\n",
    "    # Test with first sample\n",
    "    sample = X_processed[0:1]\n",
    "    prediction = model.predict(sample)\n",
    "    probability = model.predict_proba(sample)[0]\n",
    "    \n",
    "    print(\"\\nQuick Test Prediction:\")\n",
    "    print(f\"  Sample prediction: {'Malicious' if prediction[0] == 1 else 'Benign'}\")\n",
    "    print(f\"  Probability [Benign, Malicious]: [{probability[0]:.4f}, {probability[1]:.4f}]\")\n",
    "    print(f\"  Actual label: {'Malicious' if y_processed[0] == 1 else 'Benign'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyber (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
